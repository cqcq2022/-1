一、线性回归

普通线性回归的目标是求出能最小化响应变量观测值与预测值之间的误差平方和（SSE）

其中：是结果变量，是这一观测的结果变量的预测值。从而可以求得（每个预测变量的系数向量）

上述方程具有唯一解的条件为：

1）没有任何一个预测变量能表达为其他一个或多个解释变量的线性组合；

2）样本量大于预测变量的个数。

如果数据不具备以上任何一个条件，回归系数就不具备唯一解。

当利用R语言拟合线性模型时，如果预测变量间存在多重共线性，R会通过移除变量（按照他们在模型公式中的反向顺序）来拟合一个最大的可识别的模型。这是因为，即使解释变量间存在多重共线性，线性回归模型依旧可用于预测。但由于决定预测变量的回归系数不具有唯一性，故模型的系数不具备解释模型的能力。

多元线性回归模型的缺点：

1）多元线性回归模型的解对于其参数是线性的，意味着得到的解是一个超平面。如果数据具有曲线或者非线性的结构，回归模型将无法刻画这些特征。

2）多元线性回归模型易受离群点的影响。为了最小化SSE，线性回归将不得不改变参数的估计，以迁就那些不寻常的观测值。

二、偏最小二乘法（PLS）

为了解决线性回归的问题，通常的解决办法是对预测变量进行预处理，包括：

1）删除高度相关的预测变量。

2） 对预测变量进行主成分分析（PCA）得到新的预测变量，这些新的预测变量以及它们的线性组合，都是不相关的。在回归之前进行PCA预处理的方法被称为主成分回归（PCR）。这种方法在预测变量之间有内在的强相关关系，或者预测变量数大于观测值数时被广泛使用。但是PCR方法建立的预测模型可能存在一定的误导性，即PCA降维得到的新预测变量并不一定能很好的解释响应变量。因为PCA在选择成分时没有考虑到响应变量的信息，它只是简单的追踪预测变量所在空间的变异情况。

     如果预测变量与响应变量的变异相关，作者推荐使用PLS来解决那些存在相关预测变量并且希望用线性回归来加以解决的问题。

    PLS与PCA类似，都是求预测变量的线性组合。这些线性组合通常称为成分或者潜变量。PCA的线性组合将最大程度地概括预测变量空间的变异性，而预测变量PLS线性组合则最大化其与响应变量的协方差。这意味着PLS在寻找成分时，不仅要最大限度的地概括预测变量的变异，同时还要使得这些成分与响应变量的相关系数达到最大。因此，PLS在两个目标之间达到一个平衡：对预测变量空间进行降维，以及保持预测变量与响应变量之间的预测关系。换言之，PLS是一种有监督的降维方法，而PCR是无监督的降维方法。

三、岭回归

    在标准假设下，普通最小二乘回归估计的系数是无偏的，同时还是在所有线性无偏估计中方差最小的。在上一篇文章中，提到过衡量模型效果的一个标准是最小化MSE的值，但是MSE是方差和偏差的一个组合。我们经常会发现，在某些模型中，偏差的微小增长可以带来方差显著的减小，从而得到比普通最小二乘法更小的MSE。当预测变量高度相关时，估计量的方差可能会非常大，因而对于回归模型中的共线性问题，有偏模型可能得到更小的MSE。当模型出现过度拟合或出现共线性时，线性回归参数的估计可能会出现膨胀的现象。对此，我们希望控制这些估计值的规模，以减小SSE。当参数估计值变得很大时，可以通过向SSE中添加惩罚想的方法来控制参数的估计。岭回归在回归系数的平方和前面添加惩罚项参数：

 
     系数值只有在成比例的减少SSE的值时才可以取很大的值。这一方法将使得估计值向0收缩，并且随着惩罚参数的增加，收缩的程度也将增大。但无论对于任何惩罚参数，它都不能将系数的取值严格变为0.

在模型中添加了惩罚项后，我们的模型将在方差和偏差之间进行权衡，通过牺牲一些无偏性，方差往往可以大幅度的缩减，从而MSE会低于无偏模型。

四、Lasso回归   

Lasso回归是在岭回归的基础上改进的，其SSE为：


     Lasso回归改进的意义在于，在某些的取值下，参数将会严格的变为0.因此Lasso回归在对模型进行控制的时候，还能够进行变量选择。

Lasso模型的一个扩展便是弹性网模型，该模型将两项惩罚项进行了组合：


      该模型的一大优势在于，它有效的结合了正则化（通过岭回归罚）和变量选择功能（Lasso罚），这一模型对成组的高度相关的预测变量更加有效。
